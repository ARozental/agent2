config:
    sequence_lengths = [16, 16, 6, 3, 4]
    vector_sizes = [32, 48, 64, 96, 128, 156]
    num_heads = [2, 2, 2, 2, 2, 2]
    batch = 64 sentences


GPU util: 2378MiB / 11018MiB |     34%      Default
level 0 learns well from the start, except Coherence
level 1 only learns join&EoS from the start and a bit of reconstruction_diff but not reconstruction
at 45K steps 90% of loss is reconstruction1 and mlm1
at 200k steps words are reconstructed right; MLM 0 and 1 are stuck


------------------------------------------------------------------------------------------------------------------------
4 pndb experiments level1
config:
    sequence_lengths = [16, 16, 6, 3, 4]
    vector_sizes = [32, 64, 128, 96, 128, 156]
    num_heads = [4, 8, 2, 2, 2, 2]
    batch = 64 sentences
    num_transformer_layers = [4, 6, 2, 2, 2, 2]

GPU util: 7403MiB / 11018MiB |     83%      (2 models per GPU)

Coherence bug fixed, it learns now. coherence/0 => 0.01 and stays there; probably an very low grad issue (increase)

MLM bug fixed task is harder now ; also rate is 0.15 from 0.5 in last experiment.

2 of the models (no pndb, pndb1) almost exploded and came back. lost 100K worth of learning (todo1 might improve it)

reconstruction/0 (at batch 140k) learns much slower compared to the last versiob that had bugs in MLM and coherence (why???)
potential reasons:
1. MLM at 0.15 doesn't do enough to help
2. buggy MLM did something good???
3. weights that are effected by coherence now learn slower (not likely)


*batch 200k: reconstruction/0 doubles but reconstruction_diff/0 doesn't change (supports todos 3)

conclusions (todos):
1. divide reconstruction and MLM by log(len(embedding)) that way they have the same scale for each level
2. have MLM diff?
3. make sure MLM_diff and reconstruction_diff are much smaller than MLM and reconstruction (divide by 100 ???)
4. coherence should be bigger (or other stuff smaller)
5/(0). have an experiment to check the slower reconstruction reason
6. hide 1 MLM???


------------------------------------------------------------------------------------------------------------------------
exp v2

needed reconstruction/0 = 0.05
needed reconstruction/1 < 3.0

conclusions (todos):
1. bring back the max EoS loss - change it to max type loss - (remove clipping??) check if explodes
2. fix reconstruction loss 0 => done
3. move texts = [x[0] for x in texts_md5s[:64]] to config
4. make something smaller (words or char transformer layers probably; save on space)?
------------------------------------------------------------------------------------------------------------------------
reconstruction losses:
AdamW 0.001 seems best now
reconstruction losses help everything but EoS1 which oscillates; *=3 for reconstruction-EoS doesn't help
run stops at 40.8K batches of max size = 8*256
size does matter => bigger is better
------------------------------------------------------------------------------------------------------------------------
fixed 2 bugs in MLM (+layer norm and new act)

does reconstruction_mlm forces the model to "leak" the word lebal on close words? does it help or hinder reconstruction?
1. it gives reconstruction an incentive to not be perfect (leaking words to make rmlm easy)
2. it gives reconstruction a constreint to provide something that would react the same to the encoder
3. (2) is not as powerful as reconstruction_diff (to know the exact right vectors) but much much safer

layer norm and new act => at least one of them helped before bug fix, not sure now; nothing is certain now...
no idea if rmlm and/or mlm_diff help or harm after the fix, need more experiments ;( => much more gpus
------------------------------------------------------------------------------------------------------------------------
re loss should be lower or only effect the reconstruction weights => it kills regular eos loss