config:
    sequence_lengths = [16, 16, 6, 3, 4]
    vector_sizes = [32, 48, 64, 96, 128, 156]
    num_heads = [2, 2, 2, 2, 2, 2]
    batch = 64 sentences


GPU util: 2378MiB / 11018MiB |     34%      Default
level 0 learns well from the start, except Coherence
level 1 only learns join&EoS from the start and a bit of reconstruction_diff but not reconstruction
at 45K steps 90% of loss is reconstruction1 and mlm1
at 200k steps words are reconstructed right; MLM 0 and 1 are stuck


------------------------------------------------------------------------------------------------------------------------
4 pndb experiments level1
config:
    sequence_lengths = [16, 16, 6, 3, 4]
    vector_sizes = [32, 64, 128, 96, 128, 156]
    num_heads = [4, 8, 2, 2, 2, 2]
    batch = 64 sentences
    num_transformer_layers = [4, 6, 2, 2, 2, 2]

GPU util: 7403MiB / 11018MiB |     83%      (2 models per GPU)

Coherence bug fixed, it learns now. coherence/0 => 0.01 and stays there; probably an very low grad issue (increase)

MLM bug fixed task is harder now ; also rate is 0.15 from 0.5 in last experiment.

2 of the models (no pndb, pndb1) almost exploded and came back. lost 100K worth of learning (todo1 might improve it)

reconstruction/0 (at batch 140k) learns much slower compared to the last versiob that had bugs in MLM and coherence (why???)
potential reasons:
1. MLM at 0.15 doesn't do enough to help
2. buggy MLM did something good???
3. weights that are effected by coherence now learn slower (not likely)


*batch 200k: reconstruction/0 doubles but reconstruction_diff/0 doesn't change (supports todos 3)

conclusions (todos):
1. divide reconstruction and MLM by log(len(embedding)) that way they have the same scale for each level
2. have MLM diff?
3. make sure MLM_diff and reconstruction_diff are much smaller than MLM and reconstruction (divide by 100 ???)
4. coherence should be bigger (or other stuff smaller)
5/(0). have an experiment to check the slower reconstruction reason
6. hide 1 MLM???


------------------------------------------------------------------------------------------------------------------------
exp v2

needed reconstruction/0 = 0.05
needed reconstruction/1 < 3.0

conclusions (todos):
1. bring back the max EoS loss - change it to max type loss - (remove clipping??) check if explodes
2. fix reconstruction loss 0 => done
3. move texts = [x[0] for x in texts_md5s[:64]] to config
4. make something smaller (words or char transformer layers probably; save on space)?
------------------------------------------------------------------------------------------------------------------------
reconstruction losses:
AdamW 0.001 seems best now
reconstruction losses help everything but EoS1 which oscillates; *=3 for reconstruction-EoS doesn't help
run stops at 40.8K batches of max size = 8*256
size does matter => bigger is better
------------------------------------------------------------------------------------------------------------------------
fixed 2 bugs in MLM (+layer norm and new act)

does reconstruction_mlm forces the model to "leak" the word lebal on close words? does it help or hinder reconstruction?
1. it gives reconstruction an incentive to not be perfect (leaking words to make rmlm easy)
2. it gives reconstruction a constreint to provide something that would react the same to the encoder
3. (2) is not as powerful as reconstruction_diff (to know the exact right vectors) but much much safer

layer norm and new act => at least one of them helped before bug fix, not sure now; nothing is certain now...
no idea if rmlm and/or mlm_diff help or harm after the fix, need more experiments ;( => much more gpus
------------------------------------------------------------------------------------------------------------------------
re loss should be lower or only effect the reconstruction weights => it kills regular eos loss
small explosion after 37k even with lr=0.0001; didn't happen

reconstruction1 is too low but still have bad words
reconstruction_coherence doesn't work well, too easy, the decodedr generates whatever that doesn't look like real words
=> each replacement is easy to see; just a guess; see results later

=> use discriminator on reconstruction
=> discriminate between vectors, reconstructed, shuffled_vectors (shuffle the n-th position between all vectors)
=> 0.0005 is too high; 0.0002 seems ok

* not sure the discriminator helps at all => check
* maybe we should only backprop on the reconstruction weights => check
* need to also have dummy lvl1 reconstruction (select word from embedding and use it)
    => just to see nothing is broken
    => maybe run it locally?? dummy vs full see the nodes, see no error.


* print after backprop => fix
* add small norm1 (norm2+0.1*morm1) to the diff losses?? diff losses stop learning close to their 0
   => naybe not... explosion was probably caused by mlm_diff1 going to 0. shrtly after the reconstruction_diff also goes down

=> reconstruction0 with loss at 0.08 there are still errors on hard words:
The Parliament of Andalusia is => The Bareiement of Andarosia is



I like big butts too.
I like big butts as well.
=> should have the same vector because of MLM but the decompressor won't find the right EoS
=> lowering lr 0.00025=>0.0001 at 110K prevented explosion and improved learning


=> coherence_discriminator/0??  exploaded at 115k OR maybe when reconstruction is too low nothing stops reconstruction_diff??
    => reconstruction_diff*=reconstruction (use .detach or .nograd or nothing???)
=> get continuous mask from cumsum on EoS classifer; when 100% sure it acts as regular mask with frozen EoS
=> decoder block
=> get EoS loss for backprop

=>CNN5 works but why? it is kinda shitty and expects odd words after EoS/16_words in the current config. => work from reconstructed_matrices
=> look at (change?) the default LSTM cell
=> fix vec_to_children_vecs fails to decode when torch.argmax(eos_mask, dim=-1) is 0

=> is reconstruction_mlm bad because it insentivizes the decoder to "spill" the vector? should it only backprop on encoder?
=> did the grad killing stopped backprop where it shouldn't have??


=>experiment 30 exploded fast, probably due to d0 (only it went down) OR higher clipping (3.0 instead of 0.99); maybe some other issues?


=> continue adam from 230k with new mlm_diff loss, half lr and grad acc 10