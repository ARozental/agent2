
Pre Processing:
    have the dataset-generator accept the Agent level as a parameter and flatten the input accodringly: => done, needs tests
        meaning that if we have a batch with 2 paragraphs that have 3 sentences each it'll turn in to a batch of 6 sentences if Agent level is 1 (and not 2).
        For reference Agent 0 creates up to word vectors, Agent 1 creates up to sentence vectors, etc...
        Also we should do it after we decide between option 1 and 2
    padding: => done, needs tests
        all words are of max_word_length, padded with a pad token
        sentences (and above) are joined togather greedily:
            if we have 6 sentences of length [3,6,4,7,5,4] and max_words_in_sentence is 10 we make it be 4 "sentences" like this [3+6,4,7,5+4]
        when sentences are joined a <join> token is added between the words of the first and the second
        input after padding for level 1 Agent with 2 sentences: ["I am me","you are you"] looks like:
            [[3,<pad>,<pad>,<pad>],[4,5,<pad>,<pad>],[5,6,<pad>,<pad>],<level1_join_token>,[7,8,9,<pad>],[10,11,12,<pad>],[7,8,9,<pad>]]
            the 2 sentences became 1 training example because they are short

    create a DvtNode class with: pre_order_id, parent, children level, DvtTree, vector, realized? and anything else needed => done, needs tests
    create a DvtTree class with: root, level_nodes: {level (int): [all nodes in this level sorted by pre_order_id]} => done, needs tests
    all nodes in the batch have unique ids, all nodes in a tree have sequential ids. => done, needs tests
    for each example create a python object of the tree and all its nodes:
        leaves and <join> tokens are realized when their token is embedded.


    write a function to realize the tree efficiently => done for words, needs testing

    change special tokens to be 1,2,3 for pad,eos,join instead of -1,-2,-3; 0 is saved for <unk>; mask and sep tokens shouldn't be there =>


Transformer Model:
have a basic transformer with a fixed EoS token (for both encode and decode)
use SwiGLU activation function: https://arxiv.org/pdf/2102.11972.pdf
have a universal transformer => a lot later, as it is ~6X slower than the regular one for the same number of params

Main Loop:
    change the ReconstructionLoss and CoherenceLoss functions to get a batch of vectors (not the lower level ones) as an input, only MLM will get a batch of children matrices
    create batches of controllable (set from config) memory footprint from all Tree nodes for each level:
    realize  => Done
    for the first Agent level, put all "word" token sequences in a stack.
    consume it in batches that fit in memory to create all word vectors
    join all the word vectors


TEST THAT THE MODEL STILL RUNS WITH THE NEW ARCHITECTURE


AgentModel:
write actual decode function (after fixed EoS); decide how to locate the EoS token (closest vec / first close vec / have the last embedding matrix and choose first stop / other option)
write the generator
write the discriminator
write (and design?) the PNDB
write lossless compression+decompression
write style GAN => Harry Potter and the Starship Enterprise


Unit Tests:
tensor board
***to add here***

Stuff to Refactor:
***to add here***
