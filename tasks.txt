Datasets:
1. load data set from file: ["I like big butts. I can not lie.", "some other song"] => done
2. set max level option when loading the dataset - example: => done
    2 => ["I like big butts. I can not lie.", "some other song"]
    1 => ["I like big butts.", "I can not lie.", "some other song"]
    0 => ["I", "like", "big", "butts.", "I", "can", "not", "lie.", "some", "other", song"] => done
3. make a medium dataset from ../datasets/no_titles => done
4. make wiki dataset => done
5. have batch size for level instead of just level 4 batch size
6. find out who to get a real dataset (all of Kindle with style and star rating)

Transformer Model:
2. have a universal transformer => a lot later, as it is ~6X slower than the regular one for the same number of params

Training:
1. create batches of controllable (set from config) memory footprint from all Tree nodes for each level:
3. have a remove outlayer function??

Refactoring:
5. use the same name for hidden_size/vector_size/embed_size
8. make config an object again
12. fix step/global_step for grad_acc
13. when starting a job on gpu1, (with late start?) gpu0 also starts a new process with ~1gb

CURRENT TASKS:
big batch refactor
loss average loss over batch_acc number
run with 8 cores, see results
save model in gs://agent_output/models
read wiki from bucket my it private
add tpu util tracking to tensorboard
reconstructed text on tpu
fix get_children so it'll work again with level2+
go over new pndb logic together; there is no way it actually does what I hope it does
set max num words from out side, using max possible doesn't scale well => low priority
start grad_acc only after <config, 500k?> batches
in dataset logic filter out bad batches, find out why after 4k batchs we got bad pndb; delete the raise error from graph; put print for it in pre processing
why colab TB no show??
multi-gpu

AgentModel:
1. refactor actual decode function
2. write the cnn-discriminator
3. write (and design?) the PNDB => after medium dataset
4. write lossless compression+decompression => after medium dataset
5. write style GAN => Harry Potter and the Starship Enterprise
6. write level 0 coherence/mlm that accounts for generated real words?
7. install a version that uses torch.vmap(func, in_dims=0, out_dims=0)


Tests:
1. tensor board - reconstructed next to real text for TB print
2. track memory usage
3. track model norm (maybe per component if it gets out of hand)

