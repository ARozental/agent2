Datasets:
1. load data set from file: ["I like big butts. I can not lie.", "some other song"]
2. set max level option when loading the dataset - example:
    2 => ["I like big butts. I can not lie.", "some other song"]
    1 => ["I like big butts.", "I can not lie.", "some other song"]
    0 => ["I", "like", "big", "butts.", "I", "can", "not", "lie.", "some", "other", song"]
3. make a medium dataset from ../datasets/no_titles
4. find out who to get a real dataset (all of Kindle with style and star rating)

Transformer Model:
1. use SwiGLU activation function: https://arxiv.org/pdf/2102.11972.pdf
2. have a universal transformer => a lot later, as it is ~6X slower than the regular one for the same number of params

Training / refactoring:
1. create batches of controllable (set from config) memory footprint from all Tree nodes for each level:
2. find a set of sane hyper parameters
3. make code faster, avoid everyplace that exits the GPU


CURRENT TASKS:
Datasets 1,2
Transformer 1
Tests 1
AgentModel 1,2

AgentModel:
1. refactor actual decode function
2. write the generator
3. write the discriminator
4. write (and design?) the PNDB
5. write lossless compression+decompression => after medium dataset
6. write style GAN => Harry Potter and the Starship Enterprise


Tests:
1. tensor board
