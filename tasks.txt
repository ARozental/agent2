Datasets:
1. load data set from file: ["I like big butts. I can not lie.", "some other song"] => done
2. set max level option when loading the dataset - example: => done
    2 => ["I like big butts. I can not lie.", "some other song"]
    1 => ["I like big butts.", "I can not lie.", "some other song"]
    0 => ["I", "like", "big", "butts.", "I", "can", "not", "lie.", "some", "other", song"] => done
3. make a medium dataset from ../datasets/no_titles => done
4. make wiki dataset => done
5. have batch size for level instead of just level 4 batch size
6. find out who to get a real dataset (all of Kindle with style and star rating)

Transformer Model:
2. have a universal transformer => a lot later, as it is ~6X slower than the regular one for the same number of params

Training:
1. create batches of controllable (set from config) memory footprint from all Tree nodes for each level:
3. have a remove outlayer function??

Refactoring:
5. use the same name for hidden_size/vector_size/embed_size
8. make config an object again
12. fix step/global_step for grad_acc
13. when starting a job on gpu1, (with late start?) gpu0 also starts a new process with ~1gb

CURRENT TASKS:
fix get_children so it'll work again with level2+ => probably works
go over new pndb logic together; there is no way it actually does what I hope it does
in dataset logic filter out bad batches => currently done in model
why colab TB no show??
multi-gpu
fix tpu
kill annoying <unk> token for wiki dataset
FIX BUG OF NOT READING FROM CONFIG FILE BUT FROM MAIN CONFIG INSTEAD!!!! (node_sizes, save every works...)
do we need grad_norm now?


change config to reflect real numbers for level batch use different version for gpu/tpu??
    better: have max memory instead of menually finding max size per level in each config


AgentModel:
1. refactor actual decode function
2. write the cnn-discriminator
3. write (and design?) the PNDB => after medium dataset
4. write lossless compression+decompression => after medium dataset
5. write style GAN => Harry Potter and the Starship Enterprise
6. write level 0 coherence/mlm that accounts for generated real words?
7. add noise (instead of dropout) for X% of word vectors, add noise before reconstructing  => learn how to push close vectors to the right place in reconstruxtion

Tests:
1. tensor board - reconstructed next to real text for TB print
2. track memory usage
3. track model norm (maybe per component if it gets out of hand)
4. make sandbox file to load and test existing models:
    gets model file and text return vector?
    generate text
    king-man+woman
    kings-king+queen
    see how much each word is effected by pndb

